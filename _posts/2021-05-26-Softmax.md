---
layout: post
mathjax: true
title:  "Dense layer with backpropagation in C++, part 5"
date:   2021-05-21 00:00:00 +0000
categories: github jekyll
---

### Adding Softmax layer

In this post I'll modify the previous example adding Softmax layer.

### Forward path for the layer is Softmax function

Softmax function for M-dimentional input vector X is defined as

$$ \sigma {X_{i}} = $$

where X is input vector of size M:

$$ X = \left( \begin{array}{ccc}
x_{0} & x_{1} & \ldots & x_{M-1} \\
\end{array} \right)
$$

### Let's find Softmax derivative required for backward path




Previous example can be found at ["Dense layer with backpropagation and sigmoid activation in C++"] [previous_post]

[previous_post]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Sigmoid.html
[python_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense5.py
[cpp_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense5.cpp

