---
layout: post
mathjax: true
title:  "Dense layer with backpropagation in C++, part 5"
date:   2021-05-21 00:00:00 +0000
categories: github jekyll
---

### Adding Softmax layer

In this post I'll modify the previous example adding Softmax layer.

### Forward path for the layer is Softmax function

Softmax function produces M-dimentional vector output $$ \sigma {(\boldsymbol{x})} $$ for M-dimentional input vector X, and is defined as

$$ \sigma {(\boldsymbol{x})_{i}} = \frac {e^{x_{i}}} {\sum_{i=1}^{M} e^{x_{i}}} $$

where X is input vector of size M:

$$ X = \left( \begin{array}{ccc}
x_{1} & x_{2} & \ldots & x_{M} \\
\end{array} \right)
$$

### Let's find Softmax derivative required for backward path

Derivative of softmax function is a Jacobian of first order partial derivatives

$$ J = \left( \begin{array}{ccc}
\frac {\partial {\sigma_{1}}} {\partial x_{1}} & \frac {\partial {\sigma_{1}}} {\partial x_{2}} & \ldots & \frac {\partial {\sigma_{1}}} {\partial x_{M}} \\
\frac {\partial {\sigma_{2}}} {\partial x_{1}} & \frac {\partial {\sigma_{2}}} {\partial x_{2}} & \ldots & \frac {\partial {\sigma_{2}}} {\partial x_{M}} \\
\vdots & \vdots & \ldots & \vdots \\
\frac {\partial {\sigma_{M}}} {\partial x_{1}} & \frac {\partial {\sigma_{M}}} {\partial x_{2}} & \ldots & \frac {\partial {\sigma_{M}}} {\partial x_{M}} \\
\end{array} \right)
$$


Making

$$
\sigma {(\boldsymbol{x})_{i}} = \frac {g_{i}} {h}
$$

where

$$
g_{i} = e^{x_{i}} \\

h(x) = {\sum_{i=1}^{M} e^{x_{i}}}

$$

Taking derivative

$$
\frac {\partial {\sigma_{i}}} {\partial x_{j}} = \frac {\frac {\partial {g_{i}}} {\partial {x_{j}}} * h - \frac {\partial {h}} {\partial {x_{j}}} * g_{i}} {h^2}
$$


$$
\frac {\partial {g_{i}}} {\partial {x_{j}}} =
\begin{cases}
e^{x_{j}}, i = j \\
0, i \ne j
\end{cases}
$$

$$
\frac {\partial {h}} {\partial {x_{j}}} = e^{x_{j}}
$$

Using Kroneker delta

$$
\delta_{ij} =
\begin{cases}
0, i \ne j \\
1, i = j
\end{cases}
$$

$$
\frac {\partial {g_{i}}} {\partial {x_{j}}} = \delta_{ij} * e^{x_{j}}
$$

$$
\frac {\partial {\sigma_{i}}} {\partial x_{j}} = \frac {\delta_{ij} * e^{x_{j}} * h - e^{x_{j}} * e^{x_{i}}} {h^2}
$$

Simplifying

$$
\frac {\partial {\sigma_{i}}} {\partial x_{j}} = \frac {e^{x_{j}}} {h} * \frac {\delta_{ij} * h -  e^{x_{i}}} {h}
$$

$$
\frac {\partial {\sigma_{i}}} {\partial x_{j}} = \sigma_{j}  *  ({\delta_{ij} -  \sigma_{i}})
$$


$$ J = \left( \begin{array}{ccc}
\sigma_{1}*(1 - \sigma_{1}) & -\sigma_{1} * sigma_{2} & \ldots & - \sigma_{1} * \sigma_{M} \\
-\sigma_{2}*\sigma_{1} & \sigma_{2}*(1 - \sigma_{2}) & \ldots & -\sigma_{2}*\sigma_{M} \\
\vdots & \vdots & \ldots & \vdots \\
-\sigma_{M}*\sigma_{1} & -\sigma_{M}*\sigma_{2} & \ldots & - \sigma_{M}*(1 - \sigma_{M}) \\
\end{array} \right)
$$


Previous example can be found at ["Dense layer with backpropagation and sigmoid activation in C++"] [previous_post]

[previous_post]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Sigmoid.html
[python_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense5.py
[cpp_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense5.cpp

