---
layout: post
mathjax: true
title:  "Dense layer with backpropagation in C++, part 5"
date:   2021-05-21 00:00:00 +0000
categories: github jekyll
---

### Adding Softmax layer

In this post I'll modify the previous example adding Softmax layer.

### Forward path for the layer is Softmax function

Softmax function produces M-dimentional vector output $$\sigma {(\boldsymbol{x}) $$ for M-dimentional input vector X, and is defined as

$$ \sigma {(\boldsymbol{x})_{i}} = \frac {e^{x_{i}}} {\sum_{i=0}^{M-1} e^{x_{i}}} $$

where X is input vector of size M:

$$ X = \left( \begin{array}{ccc}
x_{0} & x_{1} & \ldots & x_{M-1} \\
\end{array} \right)
$$

### Let's find Softmax derivative required for backward path

Derivative of softmax function is a Jacobian of first order partial derivatives

$$ X = \left( \begin{array}{ccc}
\frac {\partial {\sigma}} {\partial x_{1}} & \ldots & \frac {\partial {\sigma}} {\partial x_{M-1}} \\
\end{array} \right)
$$



Previous example can be found at ["Dense layer with backpropagation and sigmoid activation in C++"] [previous_post]

[previous_post]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Sigmoid.html
[python_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense5.py
[cpp_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense5.cpp

