---
layout: post
mathjax: true
title:  "DNN with backpropagation in C++, part 6"
date:   2021-06-06 00:00:00 +0000
categories: github jekyll
---

### Cross-entropy Error

In this post I'll modify the previous example, adding Cross-entropy Error layer.

For classification problems it's common to use Softmax layer which predicts discrete probability
distribution for classes, together with cross-entropy error function.

### Forward path for crosss-entropy layer

Cross entropy error is defined as:

$$

E(\hat Y, Y) = - \sum_{i=9}^{M-1} \hat y_{i} * log(y_{i})

$$


Where $$ Y $$ is multi class probability vector:

$$ Y = \left( \begin{array}{ccc}
y_{0} & y_{1} & \ldots & y_{N-1} \\
\end{array} \right)
$$

$$ \hat Y $$ is expected prediction vector:

$$ \hat Y = \left( \begin{array}{ccc}
\hat y_{0} & \hat y_{1} & \ldots & \hat y_{N-1} \\
\end{array} \right)
$$

### Backward path for crosss-entropy layer

Crossentropy derivative is

$$

\frac {\partial E(\hat Y, Y)} {\partial y_{i}} = \frac {\partial (\hat y_{i} * log(y_{i}))} {\partial y_{i}} \\
\frac {\partial E(\hat Y, Y)} {\partial y_{i}} = \frac {\hat y_{i}} {y_{i}}

$$

### First I'll modify python code from previous example to use bias in dense layers.

For this experiment I've used the following software versions:

{% highlight bash %}
$ python3 -m pip freeze | grep "numpy\|tensorflow"
numpy==1.19.5
tensorflow==2.5.0rc2

$ g++ --version
g++ 9.3.0

{% endhighlight %}

Python code will be very similar to Python sample from the prevoius post.
The only change will be to replace Keras MeanSquredError with jeras.CategoricalCrossentropy


{% highlight python %}

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import RMSprop
import numpy as np

num_inputs = 2
num_outputs = 2

{% endhighlight %}

Create 3 layer sequential model

{% highlight python %}

num_inputs = 2
num_outputs = 2

# Create sequential model
model = tf.keras.Sequential()

layer1 = Dense(units=num_inputs, use_bias=True, activation="sigmoid", weights=[np.ones([num_inputs, num_inputs]), np.full([num_inputs], 2.0)])
layer2 = Dense(units=num_outputs, use_bias=True, activation=None, weights=[np.array([[1.0, 2.0], [3.0, 2.0]]), np.array([1.0, 2.0])])
layer3 = Softmax(axis=-1)
model.add(layer1)
model.add(layer2)
model.add(layer3)

{% endhighlight %}

Use cross-entropy for the loss function.

{% highlight python %}
# use CCE as loss function
loss_fn = tf.keras.losses.CategoricalCrossentropy()

{% endhighlight %}

Hardcode model input and expected model output. We'll use the same array values later in C++ implementation.

{% highlight python %}

# Arbitrary model input
x = np.array([-1.0, 1.0])

# Expected output
y_true = np.array([1.0, 0.0])

{% endhighlight %}


Use Stochastic Gradient Decent (SGD) optimizer.

SGD weight update rule is
$$
W = W - LR * \nabla
$$

$$\nabla$$ is weight gradient and $$LR$$ is learning rate.

For now we'll assume learning rate equal to 1.0

{% highlight python %}
# SGD update rule for parameter w with gradient g when momentum is 0 is as follows:
#   w = w - learning_rate * g
#
#   For simplicity make learning_rate=1.0
optimizer = tf.keras.optimizers.SGD(learning_rate=1.0, momentum=0.0)
{% endhighlight %}

In the training loop we'll compute model output for input X, compute and backpropagate the loss.

{% highlight python %}

# Get model output y for input x, compute loss, and record gradients
with tf.GradientTape(persistent=True) as tape:

    # get model output y for input x
    # add newaxis for batch size of 1
    xt = tf.convert_to_tensor(x[np.newaxis, ...])
    tape.watch(xt)
    y = model(xt)

    # obtain loss
    loss = loss_fn(y_true, y)

# adjust Dense layer weights
grad = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(grad, model.trainable_variables))

{% endhighlight %}

Finally we'll print inputs, outputs, gradients, and updated Dense layer weights.

{% highlight python %}
# print model input and output excluding batch dimention
print(f"input x={x}")
print(f"output y={y[0]}")
print(f"expected output y_true={y_true}")

# print loss
print(f"loss={loss}")

# print updated dense layer weights
print("updated weights=")
for idx, layer_vars in enumerate(model.trainable_variables):
    print(f"{idx+1}) {layer_vars.name}\n{layer_vars.numpy()}")

{% endhighlight %}


### After running Python example:

{% highlight bash %}
$ python3 dense6.py

input x=[-1.  1.]
output y=[0.26894143 0.7310586 ]
expected output y_true=[[1. 0.]]
loss=1.31326162815094
updated weights=
1) dense/kernel:0
[[1.0767565 0.9232435]
 [0.9232435 1.0767565]]
2) dense/bias:0
[1.9232435 2.0767565]
3) dense_1/kernel:0
[[1.6439142 1.3560858]
 [3.6439142 1.3560858]]
4) dense_1/bias:0
[1.7310586 1.2689414]


{% endhighlight %}

Output of Python dense6.py will be used to validate the following C++ code.
Let's code the same example in C++. Again, the only change will be to add new class
implementing Crossentropy loss.

Include c++ headers for std containers, math and output.

{% highlight c++ %}
#include <cstdio>
#include <vector>
#include <algorithm>
#include <cassert>
#include <numeric>
#include <array>
#include <chrono>
#include <iostream>
#include <string>
#include <functional>
#include <array>
#include <iterator>
#include <cmath>

using namespace std;
using std::chrono::high_resolution_clock;
using std::chrono::duration_cast;
using std::chrono::microseconds;

{% endhighlight %}

Lambda will be used to pretty print inputs, outputs, and layer weights.

{% highlight c++ %}
/*
 * Print helper function
 */
auto print_fn = [](const float& x)  -> void {printf("%.1f ", x);};

{% endhighlight %}


Dense layer weights initializers, adding few more constant initializers as
compared to previous example.

{% highlight c++ %}

/*
 * Constant weight intializer
 */
const float const_zero = 0.0;
const float const_one = 1.0;
const float const_two = 2.0;
template<float const& value = const_one>
constexpr auto const_initializer = []() -> float
{
  return value;
};

{% endhighlight %}

Dense layer class template includes forward() and backward() functions.
Now there's few more options to initialize weights and bias values from supplied arrays,
similar to how it's done in Python example above.

{% highlight c++ %}
/*
 * Dense layer class template
 *
 * Parameters:
 *  num_inputs: number of inputs to Dense layer
 *  num_outputs: number of Dense layer outputs
 *  T: input, output, and weights type in the dense layer
 *  initializer: weights initializer function
 */
template<size_t num_inputs, size_t num_outputs, typename T = float, T (*weights_initializer)() = const_initializer<const_zero>, T (*bias_initializer)() = const_initializer<const_zero> >
struct Dense
{

  typedef array<T, num_inputs> input_vector;
  typedef array<T, num_outputs> output_vector;
  typedef T (*initializer)();
  vector<input_vector> weights;
  output_vector bias;
  bool use_bias = true;

  /*
   * Default dense layer constructor
   */
  Dense(bool _use_bias)
  {
    /*
     * Create num_outputs x num_inputs weights matrix
     */
    weights.resize(num_outputs);
    for (input_vector& w: weights)
      {
        generate(w.begin(), w.end(), *weights_initializer);
      }

    /*
     * Initialize bias vector
     */
    use_bias = _use_bias;
    generate(bias.begin(), bias.end(), *bias_initializer);
  }

  /*
   * Dense layer constructor from provided weigths and biases
   * Note: weights are stored transposed
   */
  Dense(const array<array<T, num_inputs>, num_outputs>& weights_init, const array<T, num_outputs>& biases_init)
  {
    /*
     * Create num_outputs x num_inputs weights matrix
     */
    for (auto weights_row: weights_init)
      {
        weights.push_back(weights_row);
      }

    /*
     * Initialize bias vector
     */
    bias = biases_init;
  }

  /*
   * Dense layer constructor from provided weigths. Bias is not used
   * Note: weights are stored transposed
   */
  Dense(const array<array<T, num_inputs>, num_outputs>& weights_init)
  {
    /*
     * Create num_outputs x num_inputs weights matrix
     */
    for (auto weights_row: weights_init)
      {
        weights.push_back(weights_row);
      }

    use_bias = false;
  }

  /*
   * Dense layer forward pass
   * Computes X * W + B
   *  X - input row vector
   *  W - weights matrix
   *  B - bias row wector
   */
  output_vector forward(const input_vector& x)
  {
    /*
     * Check for input size mismatch
     */
    assert(x.size() == weights[0].size());

    /*
     * Layer output is dot product of input with weights
     */
    output_vector activation;
    transform(weights.begin(), weights.end(), bias.begin(), activation.begin(), [x, this](const input_vector& w, T bias)
              {
                T val = inner_product(x.begin(), x.end(), w.begin(), 0.0);
                if (use_bias)
                  {
                    val += bias;
                  }
                return val;
              }
              );

    return activation;
  }

  /*
   * Dense layer backward pass
   */
  input_vector backward(const input_vector& input, const output_vector grad)
  {
    /*
     * Weight update according to SGD algorithm with momentum = 0.0 is:
     *  w = w - learning_rate * d_loss/dw
     *
     * For simplicity assume learning_rate = 1.0
     *
     * d_loss/dw = dloss/dy * dy/dw
     * d_loss/dbias = dloss/dy * dy/dbias
     *
     * dloss/dy is input gradient grad
     *
     * dy/dw is :
     *  y = w[0]*x[0] + w[1] * x[1] +... + w[n] * x[n] + bias
     *  dy/dw[i] = x[i]
     *
     * dy/dbias is :
     *  dy/dbias = 1
     *
     * For clarity we:
     *  assume learning_rate = 1.0
     *  first compute dw
     *  second update weights by subtracting dw
     */

    /*
     * Compute backpropagated gradient
     */
    vector<output_vector> weights_transposed;
    weights_transposed.resize(num_inputs);
    for (int i = 0; i < num_inputs; i++)
      {
        for (int j = 0; j < num_outputs; j++)
          {
            weights_transposed[i][j] = weights[j][i];
          }
      }

    input_vector ret;
    transform(weights_transposed.begin(), weights_transposed.end(), ret.begin(),
              [grad](input_vector& w)
              {
                T val = inner_product(w.begin(), w.end(), grad.begin(), 0.0);
                return val;
              });

    /*
     * compute dw
     * dw = outer(x, grad)
     */
    vector<input_vector> dw;
    for (auto grad_i: grad)
      {
        auto row = input;
        for_each(row.begin(), row.end(), [grad_i](T &xi){ xi *= grad_i;});
        dw.push_back(row);
      }

    /*
     * compute w = w - dw
     * assume learning rate = 1.0
     */
    transform(weights.begin(), weights.end(), dw.begin(), weights.begin(),
              [](input_vector& left, input_vector& right)
              {
                transform(left.begin(), left.end(), right.begin(), left.begin(), minus<T>());
                return left;
              });

    if (use_bias)
      {
        /*
         * compute bias = bias - grad
         * assume learning rate = 1.0
         */
        transform(bias.begin(), bias.end(), grad.begin(), bias.begin(),
                  [](const T& bias_i, const T& grad_i)
                  {
                    return bias_i - grad_i;
                  });
      }

    return ret;
  }

  /*
   * Helper function to convert Dense layer to string
   * Used for printing the layer weights and biases
   */
  operator std::string() const
  {
    std::ostringstream ret;
    ret.precision(7);

    /*
     * output weights
     */
    ret << "weights:" << std::endl;
    for (int y=0; y < weights[0].size(); y++)
      {
        for (int x=0; x < weights.size(); x++)
          {
            if (weights[x][y] >= 0)
              ret << " ";
            ret << std::fixed << weights[x][y] << " ";
          }
        ret << std::endl;
      }

    if (use_bias)
      {
        /*
         * output biases
         */
        ret << "bias:" << std::endl;
        for (auto b: bias)
          {
            if (b >= 0)
              ret << " ";
            ret << std::fixed << b << " ";
          }
        ret << std::endl;
      }

    return ret.str();
  }

  /*
   * Helper function to cout Dense layer object
   */
  friend ostream& operator<<(ostream& os, const Dense& dense)
  {
    os << (string)dense;
    return os;
  }

};

{% endhighlight %}


Sigmoid layer class template includes forward() and backward() functions.
forward() computes sigmoid(x) = 1 / (1 + exp(-x))
backward() computes mutiple of gradient and sigmoid derivative wrt input.

{% highlight c++ %}

/*
 * Sigmoid layer class template
 */
template<size_t num_inputs, typename T = float>
struct Sigmoid
{
  typedef array<T, num_inputs> input_vector;

  static input_vector forward(const input_vector& y)
  {
    input_vector ret;

    transform(y.begin(), y.end(), ret.begin(),
              [](const T& yi)
              {
                T out = 1.0  / (1.0 + expf(-yi));
                return out;
              });
    return ret;
  }

  static input_vector backward(const input_vector& y, const input_vector grad)
  {
    input_vector ret;

    transform(y.begin(), y.end(), grad.begin(), ret.begin(),
              [](const T& y_i, const T& grad_i)
              {
                T s = 1.0  / (1.0 + expf(-y_i));
                T out = grad_i * s * (1 - s);
                return out;
              });
    return ret;
  }

};

{% endhighlight %}


Softmax layer implementation includes forward and backward path.
Forward path computes Softmax(x) = exp(x) / sum (exp(x))
Backward path is dot product of input gradient and Softmax Jacobian as per math section above.

{% highlight c++ %}
/*
 * Softmax layer class template
 */
template<size_t num_inputs, typename T = float>
struct Softmax
{
  typedef array<T, num_inputs> input_vector;

  /*
   * Softmax forward function
   */
  static input_vector forward(const input_vector& x)
  {
    input_vector y;

    /*
     * compute exp(x_i) / sum(exp(x_i), i=1..N)
     */
    transform(x.begin(), x.end(), y.begin(),
              [](const T& yi)
              {
                T out = expf(yi);
                return out;
              });

    T sum = accumulate(y.begin(), y.end(), static_cast<T>(0));

    for_each(y.begin(), y.end(), [sum](T &yi){ yi /= sum;});

    return y;
  }

  /*
   * Softmax backward function
   */
  static input_vector backward(const input_vector& x, const input_vector& grad_inp)
  {
    input_vector grad_out;
    input_vector y;
    vector<input_vector> J;

    y = forward(x);
    int s_i_j = 0;

    /*
     * Compute Jacobian of Softmax
     */
    for (auto y_i: y)
      {
        auto row = y;
        for_each(row.begin(), row.end(), [y_i](T& y_j){ y_j = -y_i * y_j;});
        row[s_i_j] += y_i;
        s_i_j++;
        J.push_back(row);
      }

    /*
     * Compute dot product of gradient and Softmax Jacobian
     */
    transform(J.begin(), J.end(), grad_out.begin(),
              [grad_inp](const input_vector& j)
              {
                T val = inner_product(j.begin(), j.end(), grad_inp.begin(), 0.0);
                return val;
              }
              );

    return grad_out;

  }

};

{% endhighlight %}

Crossentropy class will need it's own forward and backward functions.


{% highlight c++ %}
/*
 * Categorical Crossentropy loss
 *
 * Forward:
 *  E = - sum(yhat * log(y))
 *
 * Parameters:
 *  num_inputs: number of inputs to loss function.
 *  T: input type, float by defaut.
 */
template<size_t num_inputs, typename T = float>
struct CCE
{

  typedef array<T, num_inputs> input_vector;

  /*
   * Forward pass computes CCE loss for inputs yhat (label) and y (predicted)
   */
  static T forward(const input_vector& yhat, const input_vector& y)
  {
    T loss = transform_reduce(yhat.begin(), yhat.end(), y.begin(), 0.0, plus<T>(),
                              [](const T& yhat_i, const T& y_i)
                              {
                                return yhat_i * logf(y_i);
                              }
                              );
    return -1 * loss;
  }

  /*
   * Backward pass computes dloss/dy for inputs yhat (label) and y (predicted):
   *
   * dloss/dy = - yhat/y
   *
   */
  static input_vector backward(input_vector yhat, input_vector y)
  {
    array<T, num_inputs> de_dy;

    transform(yhat.begin(), yhat.end(), y.begin(), de_dy.begin(),
              [](const T& yhat_i, const T& y_i)
              {
                return -1 * yhat_i / y_i;
              }
              );
    return de_dy;
  }

};

{% endhighlight %}


Finally, in the main function, we'll declare input x and expecetd output y_true arrays, containing the same values as in out Python example.
Then we'll compute forward and backward passes, and print the network output and updated weights.

{% highlight c++ %}
int main(void)
{
  const int num_inputs = 2;
  const int num_outputs = 2;
  const int num_iterations = 1000;

  array<float, num_inputs> x = {-1.0, 1.0};
  array<float, num_outputs> yhat = {1.0, 0.0};
  array<float, num_outputs> biases_init = {1.0, 2.0};
  array<array<float, num_outputs>, num_outputs> weights_init = {
    {
        {1.0, 3.0}, {2.0, 2.0}
    }
  };

  /*
   * Create dense layers and the loss
   */
  Dense<num_inputs, num_outputs, float, const_initializer<const_one>, const_initializer<const_two> > dense1(true);
  Sigmoid<num_outputs> sigmoid;
  Dense<num_outputs, num_outputs, float, const_initializer<const_one> > dense2(weights_init, biases_init);
  Softmax<num_outputs> softmax;
  CCE<num_outputs> loss_fn;

  /*
   * Compute Dense layer output y for input x
   */
  auto y1 = dense1.forward(x);
  auto y2 = sigmoid.forward(y1);
  auto y3 = dense2.forward(y2);
  auto y4 = softmax.forward(y3);

  /*
   * Copute loss for output y and label yhat
   */
  auto loss = loss_fn.forward(yhat, y4);

  /*
   * Benchmark Dense layer inference latency
   */
  auto ts = high_resolution_clock::now();
  for (auto iter = 0;  iter < num_iterations; iter++)
    {
          y1 = dense1.forward(x);
          y2 = sigmoid.forward(y1);
          y3 = dense2.forward(y2);
          y4 = softmax.forward(y3);
    }
  auto te = high_resolution_clock::now();
  auto dt_us = (float)duration_cast<microseconds>(te - ts).count() / num_iterations;

  /*
   * Print DNN input x
   */
  printf("input x=");
  for_each(x.begin(), x.end(), print_fn);
  printf("\n");

  /*
   * Print DNN output y and expected output yhat
   */
  printf("output y=");
  for_each(y4.begin(), y4.end(), print_fn);
  printf("\n");

  printf("expected output yhat=");
  for_each(yhat.begin(), yhat.end(), print_fn);
  printf("\n");

  /*
   * Print loss for output y and label yhat
   */
  printf("loss: %f\n", loss);

  /*
   * Back propagate loss
   */
  auto dloss_dy4 = mse_loss.backward(yhat, y4);
  auto dy4_dy3 = softmax.backward(y3, dloss_dy4);
  auto dy3_dy2 = dense2.backward(y2, dy4_dy3);
  auto dy2_dy1 = sigmoid.backward(y1, dy3_dy2);
  auto dy1_dx = dense1.backward(x, dy2_dy1);

  /*
   * Print updated Dense layer weights
   */
  printf("updated dense 1 layer weights:\n%s", ((string)dense1).c_str());
  printf("updated dense 2 layer weights:\n%s", ((string)dense2).c_str());

  /*
   * Print average latency
   */
  printf("time dt=%f usec\n", dt_us);

  return 0;
}

{% endhighlight %}

### After compiling and running C++ example:

{% highlight bash %}

$ g++ -o dense5 -std=c++2a dense5.cpp && ./dense5
input x=-1.0000000 0.0000000
output y=0.2689414 0.7310586
expected output yhat=1.0000000 0.0000000
loss: 0.534447
input x=-1.0000000 0.0000000
dloss/dy: -0.7310586 0.7310586
updated dense 1 layer weights:
weights:
 1.0565200  0.9434800
 1.0000000  1.0000000
bias:
 1.9434800  2.0565200
updated dense 2 layer weights:
weights:
 1.2101572  1.7898428
 3.2101572  1.7898428
bias:
 1.2874696  1.7125304
time dt=0.001000 usec

{% endhighlight %}

As one can verify, forward path output of the C++ implementation matches the Python code.
Also, weights and biases of dense layers after backpropagation match in Python and C++ code.

Previous example can be found at ["Dense layer with backpropagation, adding Softmax"] [previous_post]

Python source code for this example is at [dense6.py] [python_source_code]

C++ implementation is at [dense6.cpp] [cpp_source_code]


[previous_post]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Softmax.html
[python_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense6.py
[cpp_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense6.cpp

