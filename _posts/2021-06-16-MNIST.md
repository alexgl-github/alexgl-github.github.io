---
layout: post
mathjax: true
title:  "DNN with backpropagation in C++, part 7"
date:   2021-06-06 00:00:00 +0000
categories: github jekyll
---

### Putting it all together

So far we have build a simple DNN consisting of two Dense layers, Sigmoid activation layer and output Softmax layer.

Now let's train this DNN to recognize handwritten digit classification problem.

Previous posts can be found at:

["DNN with backpropagation in C++, part 6. Cross-entropy Error"] [part_6]

["DNN with backpropagation in C++, part 5. Adding Softmax layer"] [part_5]

["DNN with backpropagation in C++, part 4. Adding sigmoid activation"] [part_4]

["DNN with backpropagation in C++, part 3. Dense layer with backpropagation and bias"] [part_3]

["DNN with backpropagation in C++, part 2. Two layer NN with backpropagation"] [part_2]

["DNN with backpropagation in C++, part 1. Dense layer with backpropagation"] [part_1]


### MNIST dataset

For training I'll use [MNIST dataset of handwritten digit images] [MNIST], which contains 60000 training images and labels,
and 10000 image-label pairs for validation.

I'll focus on C++ implementation only, Python equivalent won't be added because theres already many MNIST classification examples
available in all major deep learning frameworks.


First, let's download the dataset and extract it into data folder.

{% highlight bash %}
$ mkdir data && cd data
$ wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
$ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
$ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
$ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
$ gunzip train-images-idx3-ubyte.gz && gunzip train-labels-idx1-ubyte.gz && gunzip t10k-images-idx3-ubyte.gz && gunzip t10k-labels-idx1-ubyte.gz
$ cd ..
{% endhighlight %}

Dataset images and labels binary files have the following format.

Labes binary file format:

{%highlight C++ %}
TRAINING SET LABEL FILE (train-labels-idx1-ubyte):
[offset] [type]          [value]          [description]
0000     32 bit integer  0x00000801       magic number
                                          (MSB first)
0004     32 bit integer  60000            number of items
0008     unsigned byte   ??               label
0009     unsigned byte   ??               label
........
xxxx     unsigned byte   ??               label
{% endhighlight %}

After header containing magic number and total number of labels in the dataset, there are label records.
Each record contains one label.
Each label has size of 1 byte and is between 0 and 9 indicating handwritten digit value.


Images binary file format:

{%highlight C++ %}
[offset] [type]          [value]          [description]
0000     32 bit integer  0x00000803(2051) magic number
                                          (MSB first)
0004     32 bit integer  10000            number of images
0008     32 bit integer  28               number of rows
0012     32 bit integer  28               number of columns
0016     unsigned byte   ??               pixel
0017     unsigned byte   ??               pixel
........
xxxx     unsigned byte   ??               pixel
{% endhighlight %}

File header contains magic number, total number of images in the dataset, and image height and width.
Each following image record contains pixel values for one image of size <height> rows by <width> pixels per row,
in row scan order format.

Pixels are one pixel per byte with values between 0 and 255.
All images are of height 28 rows and width of 28 pixels per row.

Example of MNIST image and corresponding label:

![mnist_image]({{ site.url }}/images/mnist_image.png)

Label: 6


### C++ implementaiton of MNIST dataset parser

Using this format information, we can implement C++ code mnist.h for reading MNIST images and labels.
Also note that the values are in MSB (big-endian) format, so I'll need to add MSB to LSB conversion to the dataset class.

Inclide header files used later in the implementation.

{%highlight C++ %}
#pragma once

#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <array>
#include <algorithm>
#include <cassert>
{% endhighlight %}

Add comment with format of MNIST binary files

{%highlight C++ %}
/*
 * Full dataset format specification is at http://yann.lecun.com/exdb/mnist/
 *
 * TRAINING SET LABEL FILE (train-labels-idx1-ubyte):
 * [offset] [type]          [value]          [description]
 * 0000     32 bit integer  0x00000801(2049) magic number (MSB first)
 * 0004     32 bit integer  60000            number of items
 * 0008     unsigned byte   ??               label
 * 0009     unsigned byte   ??               label
 * ........
 * xxxx     unsigned byte   ??               label
 * The labels values are 0 to 9.
 *
 * TRAINING SET IMAGE FILE (train-images-idx3-ubyte):
 * [offset] [type]          [value]          [description]
 * 0000     32 bit integer  0x00000803(2051) magic number
 * 0004     32 bit integer  60000            number of images
 * 0008     32 bit integer  28               number of rows
 * 0012     32 bit integer  28               number of columns
 * 0016     unsigned byte   ??               pixel
 * 0017     unsigned byte   ??               pixel
 * ........
 * xxxx     unsigned byte   ??               pixel
 *
 * Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).
 *
 */
{% endhighlight %}


Template C++ class for reading MNIST dataset.

It contains:

- dataset object constructor from provided image and labels binary file path;

- read_next() for reading next image and next label values;

- rewind() to reset dataset file pointer to the beginning.

- other helper functions.

{%highlight C++ %}
/*
 * MNIST dataset class
 *
 * num_classe=10 and image size=28x28 are as per dataset specifiction
 *
 * typename T=float is type of the data returned by read_next_image call
 *
 */
template<size_t num_classes=10, size_t image_size=28*28, typename T=float>
struct mnist
{
  /* Error codes returned by mnist api */
  enum error
    {
      MNIST_OK = 0,
      MNIST_EOF = -1,
      MNIST_ERROR = -2
    };

  /* images file descriptor */
  int fd_images = -1;
  /* labels file descriptor */
  int fd_labels = -1;
  /* images magic number */
  const uint32_t magic_image_value = 0x00000803;
  /* labels magic number */
  const uint32_t magic_label_value = 0x00000801;
  /* number of images in the dataset */
  int number_of_images = 0;
  /* number of labels in the dataset */
  int number_of_labels = 0;

  /*
   * Initialize MNIST dataset from image file and labels file
   * Read and verify magic numbers
   * Verify number if images matches number of labels
   * Verify read image size matches template parameter image_size
   */
  mnist(const char* path_images, const char* path_labels)
  {
    uint32_t magic_image,  magic_label;
    int rows, cols;

    /*
     * Read images header
     */
    fd_images = open(path_images, O_RDONLY);
    assert(fd_images != -1);

    read(fd_images, &magic_image, sizeof(magic_image));
    assert(endian_swap<uint32_t>(magic_image) == magic_image_value);

    read(fd_images, &number_of_images, sizeof(number_of_images));
    number_of_images = endian_swap<int>(number_of_images);

    read(fd_images, &rows, sizeof(rows));
    read(fd_images, &cols, sizeof(cols));
    rows = endian_swap<int>(rows);
    cols = endian_swap<int>(cols);
    assert(rows * cols == image_size);

    /*
     * Read labels header
     */
    fd_labels = open(path_labels, O_RDONLY);
    assert(fd_labels != -1);

    read(fd_labels, &magic_label, sizeof(magic_label));
    assert(endian_swap<uint32_t>(magic_label) == magic_label_value);

    read(fd_labels, &number_of_labels, sizeof(number_of_labels));
    number_of_labels = endian_swap<int>(number_of_labels);
    assert(number_of_images == number_of_labels);
  }

  /*
   * Close file descriptors in destructor
   */
  ~mnist()
  {
    close(fd_images);
    close(fd_labels);
  }

  /*
   * Read next image pixel values
   */
  int read_next_image(std::array<T, image_size>& data)
  {
    /*
     * Read uint_8 pixel values
     */
    std::array<uint8_t, image_size> raw_data;
    ssize_t ret = read(fd_images, raw_data.data(), image_size);
    if (ret == 0)
      {
        return MNIST_EOF;
      }

    assert(ret == image_size);

    /*
     * convert to floating point and normalize
     */
    std::transform(raw_data.begin(), raw_data.end(), data.begin(),
              [](const uint8_t x) {
                return static_cast<T>(x) / 256.0;
              }
              );
    return MNIST_OK;
  }

  /*
   * Read next image label and return on one-hot encoded formt
   */
  int read_next_label(std::array<T, num_classes>& label)
  {
    uint8_t label_index = 0;

    /*
     * Fill one hot array with zeros
     */
    std::fill(label.begin(), label.end(), 0);

    /*
     * Read label value
     */
    ssize_t ret = read(fd_labels, &label_index, sizeof(label_index));

    if (ret == 0)
      {
        return MNIST_EOF;
      }

    assert(ret == sizeof(label_index));
    assert(label_index < num_classes);

    /*
     * Set one hot array at label value index to 1
     */
    label[label_index] = 1.0;
    return MNIST_OK;
  }

  /*
   * Wrapper for read_next_image() and read_next_label()
   */
  int read_next(std::array<T, image_size>& image, std::array<T, num_classes>& label)
  {
    auto ret1 = read_next_image(image);
    auto ret2 = read_next_label(label);
    if (ret1 == MNIST_EOF || ret2 == MNIST_EOF)
      {
        return MNIST_EOF;
      }
    assert(ret1 == MNIST_OK);
    assert(ret2 == MNIST_OK);
    return MNIST_OK;
  }

  /*
   * Reset file offsets to the beginning of data
   * skipping headers
   */
  void rewind()
  {
    if (fd_images != -1 && fd_labels != -1)
      {
        /*
         * seek to data offsets in labels and images. For offsets see start of mnist.h
         */
        lseek(fd_images, 16, SEEK_SET);
        lseek(fd_labels, 8, SEEK_SET);
      }
  }

  /*
   * Utility to convert betwen big/little endiannes
   */
  template<typename S = uint32_t>
  static S endian_swap(T val)
  {
    typedef union
    {
      S val;
      unsigned char array[sizeof(S)];
    } swap_t;
    swap_t src, dst;
    src.val = val;
    for (size_t i = 0; i < sizeof(S); i++)
      {
        dst.array[i] = src.array[sizeof(S) - i - 1];
      }
    return dst.val;
  }

};
{% endhighlight %}


### F1 score

To validate training results I'll be using multi-class F1 score.
F1 score is a very common metric used to rate accuracy of a classification DNN.

Each class score is harmonic mean of precision and recall scores.
Final score is mean of all class scores.

$$
F1(class) = \frac {2 * Precision(class) * Recall(class)} {Precision(class) + Recall(class)}
$$

where

$$
Precision(class) = \frac {TruePositives(class)} {TruePositives(class) * FalsePositives(class)}
$$

$$
Recall(class) = \frac {TruePositives(class)} {TruePositives(class) * FalseNegatives(class)}
$$



{%highlight C++ %}
#pragma once
#include <array>
#include <algorithm>

/*
 * F1 score class
 *
 * Computes F1 score for multi-class classification
 *
 * f1_score = 2 * Preision * Recall / (Precision + Recall)
 *
 * TP = TruePositive
 * FP = FalsePositive
 * FN = FalseNegative
 *
 * Precision(class) = TP(class) / (TP(class) + FP(class))
 * Recall(class) = TP(class) / (TP(class) + FN(class))
 *
 *
 */
template<size_t num_classes=10, typename T=float>
struct f1
{
  std::array<int, num_classes> fp;
  std::array<int, num_classes> fn;
  std::array<int, num_classes> tp;
  static constexpr float eps = 0.000001;

  /*
   * Default f1 class constructor
   */
  f1()
  {
    reset();
  }

  /*
   * Reset accumulated TP, FP, FN counters
   */
  void reset()
  {
    fp.fill({});
    fn.fill({});
    tp.fill({});
  }

  /*
   * Update TP, FP, FN counters
   *  y_true is one-hot label
   *  y_pred is predicted probabilites vector output from softmax
   */
  void update(const std::array<T, num_classes>& y_true, const std::array<T, num_classes>& y_pred)
  {
    auto idx_true = std::distance(y_true.begin(), std::max_element(y_true.begin(), y_true.end()));
    auto idx_pred = std::distance(y_pred.begin(), std::max_element(y_pred.begin(), y_pred.end()));
    fp[idx_pred] += 1;
    fn[idx_true] += 1;
    if (idx_true == idx_pred)
      {
        tp[idx_true] += 1;
      }
  }

  /*
   * Get f1 score value as average of f1 class scores
   */
  float score()
  {

    std::array<float, num_classes> precision;
    std::array<float, num_classes> recall;
    std::array<float, num_classes> scores;

    std::transform(tp.begin(), tp.end(), fp.begin(), precision.begin(),
                   [](const int& tp_i, const int& fp_i)
                   {
                     return static_cast<float>(tp_i) / static_cast<float>(fp_i + eps);
                   });
    std::transform(tp.begin(), tp.end(), fn.begin(), recall.begin(),
                   [](const int& tp_i, const int& fn_i)
                   {
                     return static_cast<float>(tp_i) / static_cast<float>(fn_i + eps);
                   });
    std::transform(precision.begin(), precision.end(), recall.begin(), scores.begin(),
                   [](const float& precision_i, const float& recall_i)
                   {
                     auto score =  2.0 * precision_i * recall_i / (precision_i + recall_i + eps);
                     return score;
                   });

    auto score_total = std::accumulate(scores.begin(), scores.end(), static_cast<float>(0.0));
    return score_total / static_cast<float>(num_classes);
  }
};
{% endhighlight %}


### Training loop

Main C++ module implementing DNN layers, training and validation loops, and reporting loss and F1 score for train and validation datasets.

I have added some optimizations to speed up dense layer backward path.

Dense backward() function has been split into gradient backpropagation and weight update parts.
Gradient backpropagation is done on each iteration, and weight update is done once per batch, this improves compute time.

Also I keep transposed matrix of weight pointers in Dense object. It is polulated once in the Dense class constructor, and
is used in backward() function, therefore we don't need  transpose Dense layer weights in each backward() function call.

Finally, sigmoid and softmax derivatives are computed using output of sigmoid and softmax forward path. Therefore there's no
need to repeat this computation again in the sigmoid and softmax backward() functios, because we can use output of the corresponding forward() API.

{%highlight C++ %}

#include <cstdio>
#include <vector>
#include <array>
#include <algorithm>
#include <cassert>
#include <array>
#include <chrono>
#include <sstream>
#include <string>
#include <iterator>
#include <random>
#include "mnist.h"
#include "f1.h"

using namespace std;
using std::chrono::high_resolution_clock;
using std::chrono::duration_cast;
using std::chrono::microseconds;
using std::chrono::milliseconds;
using std::chrono::seconds;

/*
 * Constant weight intializer
 */
const float const_one = 1.0;
const float const_zero = 0.0;
const float const_one_half = 0.0;

template<float const& value = const_zero>
constexpr auto const_initializer = []() -> float
{
  return value;
};

/*
 * Random uniform weights initializer
 */
constexpr auto random_uniform_initializer = []() -> float
{
  /*
   * Return random values in the range [-1.0, 1.0]
   */
  return 2 * static_cast<float>(rand()) / static_cast<float>(RAND_MAX) - 1.0;
};

/*
 * Dense layer class template
 *
 * Parameters:
 *  num_inputs: number of inputs to Dense layer
 *  num_outputs: number of Dense layer outputs
 *  T: input, output, and weights type in the dense layer
 *  initializer: weights initializer function
 */
template<size_t num_inputs, size_t num_outputs, typename T = float, T (*weights_initializer)() = random_uniform_initializer, T (*bias_initializer)() = const_initializer<const_zero> >
struct Dense
{

  /*
   * input outut vector type definitions
   */
  typedef array<T, num_inputs> input_vector;
  typedef array<T, num_outputs> output_vector;

  /*
   * dense layer weights matrix W, used in y = X * W
   * weights are transposed to speed up forward() computation
   */
  vector<input_vector> weights;

  /*
   * bias vector
   */
  output_vector bias;

  /*
   * Flag to enable/disable bias
   */
  bool use_bias = true;

  /*
   * Matrix of transposed weights W pointers, used to speed up backward() path
   */
  vector<array<T*, num_outputs>> weights_transposed;
  vector<input_vector> dw;
  output_vector db;

  /*
   * Default dense layer constructor
   */
  Dense(bool _use_bias=true)
  {
    /*
     * Create num_outputs x num_inputs weights matrix
     */
    weights.resize(num_outputs);
    for (input_vector& w: weights)
      {
        generate(w.begin(), w.end(), *weights_initializer);
      }

    /*
     * Ctreate transposed array of weighst pointers
     */
    weights_transposed.resize(num_inputs);
    for (size_t i = 0; i < num_inputs; i++)
      {
        for (size_t j = 0; j < num_outputs; j++)
          {
            weights_transposed[i][j] = &weights[j][i];
          }
      }

    /*
     * Initialize bias vector
     */
    use_bias = _use_bias;
    generate(bias.begin(), bias.end(), *bias_initializer);

    /*
     * Initialize dw, db
     */
    dw.resize(num_outputs);
    reset_grads();
  }

  /*
   * Dense layer constructor from provided weigths and biases
   * Note: weights are stored transposed
   */
  Dense(const array<array<T, num_inputs>, num_outputs>& weights_init, const array<T, num_outputs>& biases_init)
  {
    /*
     * Create num_outputs x num_inputs weights matrix
     */
    for (auto weights_row: weights_init)
      {
        weights.push_back(weights_row);
      }

    /*
     * Initialize bias vector
     */
    bias = biases_init;
  }

  /*
   * Dense layer constructor from provided weigths. Bias is not used
   * Note: weights are stored transposed
   */
  Dense(const array<array<T, num_inputs>, num_outputs>& weights_init)
  {
    /*
     * Create num_outputs x num_inputs weights matrix
     */
    for (auto weights_row: weights_init)
      {
        weights.push_back(weights_row);
      }

    use_bias = false;
  }

  /*
   * Dense layer forward pass
   * Computes X * W + B
   *  X - input row vector
   *  W - weights matrix
   *  B - bias row wector
   */
  output_vector forward(const input_vector& x)
  {
    /*
     * Check for input size mismatch
     */
    assert(x.size() == weights[0].size());

    /*
     * Layer output is dot product of input with weights
     */
    output_vector activation;
    transform(weights.begin(), weights.end(), bias.begin(), activation.begin(), [x, this](const input_vector& w, T bias)
              {
                T val = inner_product(w.begin(), w.end(), x.begin(), 0.0);
                if (use_bias)
                  {
                    val += bias;
                  }
                return val;
              }
              );

    return activation;
  }

  /*
   * Dense layer backward pass
   */
  input_vector backward(const input_vector& input, const output_vector grad)
  {
    /*
     * Weight update according to SGD algorithm with momentum = 0.0 is:
     *  w = w - learning_rate * d_loss/dw
     *
     * For simplicity assume learning_rate = 1.0
     *
     * d_loss/dw = dloss/dy * dy/dw
     * d_loss/dbias = dloss/dy * dy/dbias
     *
     * dloss/dy is input gradient grad
     *
     * dy/dw is :
     *  y = w[0]*x[0] + w[1] * x[1] +... + w[n] * x[n] + bias
     *  dy/dw[i] = x[i]
     *
     * dy/dbias is :
     *  dy/dbias = 1
     *
     * For clarity we:
     *  assume learning_rate = 1.0
     *  first compute dw
     *  second update weights by subtracting dw
     */

    /*
     * Compute backpropagated gradient
     */
    input_vector grad_out;
    transform(weights_transposed.begin(), weights_transposed.end(), grad_out.begin(),
              [grad](array<T*, num_outputs>& w)
              {
                T val = inner_product(w.begin(), w.end(), grad.begin(), 0.0,
                                      [](const T& l, const T& r)
                                      {
                                        return l + r;
                                      },
                                      [](const T* l, const T& r)
                                      {
                                        return *l * r;
                                      }
                                      );
                return val;
              });

    /*
     * accumulate weight updates
     * compute dw = dw + outer(x, grad)
     */
    transform(dw.begin(), dw.end(), grad.begin(), dw.begin(),
              [input](input_vector& left, const T& grad_i)
              {
                /* compute outer product for each row */
                auto row = input;
                for_each(row.begin(), row.end(), [grad_i](T &xi){ xi *= grad_i;});

                /* accumulate into dw */
                std::transform (left.begin(), left.end(), row.begin(), left.begin(), std::plus<T>());
                return left;
              });

    /*
     * accumulate bias updates
     */
    std::transform(db.begin(), db.end(), grad.begin(), db.begin(), std::plus<T>());

    return grad_out;
  }

  /*
   * Update Dense layer weigts/biases using accumulated dw/db
   */
  void train(float learning_rate)
  {
    /*
     * compute w = w - learning_rate * dw
     */
    transform(weights.begin(), weights.end(), dw.begin(), weights.begin(),
              [learning_rate](input_vector& left, input_vector& right)
              {
                transform(left.begin(), left.end(), right.begin(), left.begin(),
                          [learning_rate](const T& w_i, const T& dw_i)
                          {
                            return w_i - learning_rate * dw_i;
                          });
                return left;
              });


    if (use_bias)
      {
        /*
         * compute bias = bias - grad
         * assume learning rate = 1.0
         */
        transform(bias.begin(), bias.end(), db.begin(), bias.begin(),
                  [learning_rate](const T& bias_i, const T& db_i)
                  {
                    return bias_i - learning_rate * db_i;
                  });
      }

    reset_grads();
  }

  /*
   * Reset weigth and bias gradient accumulators
   */
  void reset_grads()
  {
    for (input_vector& dw_i: dw)
      {
        generate(dw_i.begin(), dw_i.end(), const_initializer<const_zero>);
      }
    generate(db.begin(), db.end(), const_initializer<const_zero>);
  }

  /*
   * Helper function to convert Dense layer to string
   * Used for printing the layer weights and biases
   */
  operator std::string() const
  {
    std::ostringstream ret;
    ret.precision(7);

    /*
     * output weights
     */
    ret << "weights:" << std::endl;
    for (int y=0; y < weights[0].size(); y++)
      {
        for (int x=0; x < weights.size(); x++)
          {
            if (weights[x][y] >= 0)
              ret << " ";
            ret << std::fixed << weights[x][y] << " ";
          }
        ret << std::endl;
      }

    if (use_bias)
      {
        /*
         * output biases
         */
        ret << "bias:" << std::endl;
        for (auto b: bias)
          {
            if (b >= 0)
              ret << " ";
            ret << std::fixed << b << " ";
          }
        ret << std::endl;
      }

    return ret.str();
  }

  /*
   * Helper function to cout Dense layer object
   */
  friend ostream& operator<<(ostream& os, const Dense& dense)
  {
    os << (string)dense;
    return os;
  }

};


/*
 * Sigmoid layer class template
 */
template<size_t num_inputs, typename T = float>
struct Sigmoid
{
  typedef array<T, num_inputs> input_vector;

  /*
   * Sigmoid forward pass
   */
  static input_vector forward(const input_vector& y)
  {
    input_vector ret;

    transform(y.begin(), y.end(), ret.begin(),
              [](const T& yi)
              {
                T out = 1.0  / (1.0 + expf(-yi));
                return out;
              });
    return ret;
  }

  /*
   * Sigmoid backward pass
   */
  static input_vector backward(const input_vector& y, const input_vector grad)
  {
    input_vector ret;

    transform(y.begin(), y.end(), grad.begin(), ret.begin(),
              [](const T& y_i, const T& grad_i)
              {
                T out = grad_i * y_i * (1 - y_i);
                return out;
              });
    return ret;
  }

};


/*
 * Softmax layer class template
 */
template<size_t num_inputs, typename T = float>
struct Softmax
{
  typedef array<T, num_inputs> input_vector;

  /*
   * Softmax forward function
   */
  static input_vector forward(const input_vector& x)
  {
    input_vector y;

    /*
     * compute exp(x_i) / sum(exp(x_i), i=1..N)
     */
    transform(x.begin(), x.end(), y.begin(),
              [](const T& yi)
              {
                T out = expf(yi);
                return out;
              });

    T sum = accumulate(y.begin(), y.end(), static_cast<T>(0.0));
    for_each(y.begin(), y.end(), [sum](T &yi){ yi /= sum;});

    return y;
  }

  /*
   * Softmax backward function
   */
  static input_vector backward(const input_vector& y, const input_vector& grad_inp)
  {
    input_vector grad_out;
    vector<input_vector> J;

    /*
     * Compute Jacobian of Softmax
     */
    int s_i_j = 0;
    for (auto y_i: y)
      {
        auto row = y;
        for_each(row.begin(), row.end(), [y_i](T& y_j){ y_j = -y_i * y_j;});
        row[s_i_j] += y_i;
        s_i_j++;
        J.push_back(row);
      }

    /*
     * Compute dot product of gradient and Softmax Jacobian
     */
    transform(J.begin(), J.end(), grad_out.begin(),
              [grad_inp](const input_vector& j)
              {
                T val = inner_product(j.begin(), j.end(), grad_inp.begin(), 0.0);
                return val;
              }
              );

    return grad_out;

  }

};

/*
 * Categorical Crossentropy loss
 *
 * Forward:
 *  E = - sum(yhat * log(y))
 *
 * Parameters:
 *  num_inputs: number of inputs to loss function.
 *  T: input type, float by defaut.
 */
template<size_t num_inputs, typename T = float>
struct CCE
{

  typedef array<T, num_inputs> input_vector;

  /*
   * Forward pass computes CCE loss for inputs yhat (label) and y (predicted)
   */
  static T forward(const input_vector& yhat, const input_vector& y)
  {
    input_vector cce;
    transform(yhat.begin(), yhat.end(), y.begin(), cce.begin(),
              [](const T& yhat_i, const T& y_i)
              {
                return yhat_i * logf(y_i);
              }
              );
    T loss = accumulate(cce.begin(), cce.end(), 0.0);
    return -1 * loss;
  }

  /*
   * Backward pass computes dloss/dy for inputs yhat (label) and y (predicted):
   *
   * dloss/dy = - yhat/y
   *
   */
  static input_vector backward(input_vector yhat, input_vector y)
  {
    array<T, num_inputs> de_dy;

    transform(yhat.begin(), yhat.end(), y.begin(), de_dy.begin(),
              [](const T& yhat_i, const T& y_i)
              {
                return -1 * yhat_i / (y_i);
              }
              );
    return de_dy;
  }

};


int main(void)
{
  /*
   * Path to MNIST train and validation data
   */
  const char* train_images_path = "./data/train-images-idx3-ubyte";
  const char* train_labels_path = "./data/train-labels-idx1-ubyte";
  const char* validation_images_path = "./data/t10k-images-idx3-ubyte";
  const char* validation_labels_path = "./data/t10k-labels-idx1-ubyte";

  /*
   * Number of classes in MNIST dataset
   */
  const int num_classes = 10;

  /*
   * MNIST image dimentions
   */
  const int num_rows = 28;
  const int num_cols = 28;
  const int image_size = num_rows * num_cols;

  /*
   * Train for 5 epochs
   */
  const int num_epochs = 5;

  /*
   * Print report every 10000 iterations
   */
  const int report_interval = 10000;

  /*
   * Storage for next image and label
   */
  std::array<float, image_size> image;
  std::array<float, num_classes> label;

  /*
   * Training learning rate and batch size
   */
  float learning_rate = 0.01;
  int batch_size = 100;

  /*
   * MNIST train dataset
   */
  mnist train_dataset(train_images_path, train_labels_path);

  /*
   * MNIST validation dataset
   */
  mnist validation_dataset(validation_images_path, validation_labels_path);


  printf("train dataset: %d images and %d labels\n", train_dataset.number_of_images, train_dataset.number_of_labels);
  printf("validation dataset: %d images and %d labels\n", validation_dataset.number_of_images, validation_dataset.number_of_labels);

  /*
   * Number of iterations in epoch is number of records in the dataset
   */
  const auto iterations = train_dataset.number_of_images;

  /*
   * Create DNN layers and the loss
   */
  Dense<image_size, 128> dense1;
  Sigmoid<128> sigmoid1;
  Dense<128, num_classes> dense2;
  Softmax<num_classes> softmax;
  CCE<num_classes> loss_fn;
  f1<num_classes, float> f1_train;
  f1<num_classes, float> f1_validation;

  /*
   * shortcut for mnust error code
   */
  using mnist_error = mnist<10, 28*28, float>::error;

  /*
   * Training loop
   * Train for num_epochs
   */
  for (auto epoch=0; epoch < num_epochs; epoch++)
    {
      /*
       * Reset dataset positons, loss accuracy counters, and clocks
       */
      train_dataset.rewind();
      float loss_epoch = 0;
      f1_train.reset();
      auto ts = high_resolution_clock::now();

      /*
       * Train for number of iterations per each epoch
       */
      for (auto iter = 0; iter < iterations / batch_size; iter++)
        {

          /*
           * Repeat forward path for batch_size
           */
          for (auto batch = 0; batch < batch_size; batch++)
            {

              auto ret = train_dataset.read_next(image, label);
              if (ret == mnist_error::MNIST_EOF)
                {
                  break;
                }

              /*
               * Compute Dense layer output y for input x
               */
              auto y1 = dense1.forward(image);
              auto y2 = sigmoid1.forward(y1);
              auto y3 = dense2.forward(y2);
              auto y4 = softmax.forward(y3);
              auto loss = loss_fn.forward(label, y4);

              /*
               * Update f1 score
               */
              f1_train.update(label, y4);

              /*
               * Back propagate loss
               */
              auto dloss_dy4 = loss_fn.backward(label, y4);
              auto dy4_dy3 = softmax.backward(y4, dloss_dy4);
              auto dy3_dy2 = dense2.backward(y2, dy4_dy3);
              auto dy2_dy1 = sigmoid1.backward(y2, dy3_dy2);
              dense1.backward(image, dy2_dy1);

              /*
               * Accumulate loss for reporting
               */
              loss_epoch += loss;
            }

          /*
           * Update dense layers weights once per batch
           */
          dense2.train(learning_rate);
          dense1.train(learning_rate);

          /*
           * Capture clocks and print losses and time stats
           */
          if ( (((iter+1) * batch_size) % report_interval) == 0)
            {
              auto te = high_resolution_clock::now();
              auto dt = (float)duration_cast<seconds>(te - ts).count();
              printf("epoch=%d/%d iter=%d/%d time/iter=%.4f sec loss=%.5f f1=%.5f\n",
                     epoch,
                     num_epochs,
                     (iter+1), iterations/batch_size,
                     dt / ((iter + 1)*batch_size),
                     loss_epoch / ((iter + 1)*batch_size),
                     f1_train.score());
            }
        }

      /*
       * Capture run time per epoch
       */
      auto te = high_resolution_clock::now();
      auto dt = (float)duration_cast<seconds>(te - ts).count();

      /*
       * Average loss per epoch
       */
      loss_epoch = loss_epoch / iterations;

      /*
       * Print epoch stats
       */
      printf("epoch %d/%d time/epoch=%.5f sec; time left=%.4f hr; avg loss=%.5f; f1=%.5f\n",
             epoch,
             num_epochs,
             dt, dt * (num_epochs - epoch) / (60*60),
             loss_epoch,
             f1_train.score());


      /*
       * Validation loop
       */
      float loss_validation = 0;
      validation_dataset.rewind();
      f1_validation.reset();

      /*
       * Repeat each epoch for entire validation dataset
       */
      for (auto iter = 0; iter < validation_dataset.number_of_images; iter++)
        {
          /*
           * Read next image and label
           */
          auto ret = validation_dataset.read_next(image, label);
          if (ret == mnist_error::MNIST_EOF)
            {
              break;
            }

          /*
           * Forward path
           */
          auto y1 = dense1.forward(image);
          auto y2 = sigmoid1.forward(y1);
          auto y3 = dense2.forward(y2);
          auto y4 = softmax.forward(y3);
          auto loss = loss_fn.forward(label, y4);

          /*
           * Update validation loss and counters
           */
          loss_validation += loss;
          f1_validation.update(label, y4);
        }

      /*
       * Report validation loss and f1 score
       */
      loss_validation = loss_validation / validation_dataset.number_of_images;
      printf("epoch %d/%d validation loss: %f  f1: %.5f\n",
             epoch, num_epochs, loss_validation, f1_validation.score());
    }

  return 0;
}
{% endhighlight %}

#### Let's build the source code and run the MNIST training example

It will run for 5 epochs, processing 60000 training images and 10000 validation images in each epoch.
Cross-entropy loss should be decrementing each epoch, and accuracy F1 score should be going up.

{% highlight bash %}
$ g++ -o dense7 -Wall -std=c++2a dense7.cpp && ./dense7
train dataset: 60000 images and 60000 labels
validation dataset: 10000 images and 10000 labels
epoch=0/5 iteration=1/600 loss=6.68182 f1=0.05501
<skipped per epoch prints>
...
epoch 0/5 time/epoch=93.00000 sec; time left=0.1292 hr; avg loss=0.50935; f1=0.85821
epoch 0/5 validation loss: 0.299749  f1: 0.90644
epoch 1/5 time/epoch=94.00000 sec; time left=0.1044 hr; avg loss=0.23553; f1=0.92966
epoch 1/5 validation loss: 0.234321  f1: 0.92660
epoch 2/5 time/epoch=95.00000 sec; time left=0.0792 hr; avg loss=0.18567; f1=0.94427
epoch 2/5 validation loss: 0.203892  f1: 0.93722
epoch 3/5 time/epoch=95.00000 sec; time left=0.0528 hr; avg loss=0.15537; f1=0.95326
epoch 3/5 validation loss: 0.184529  f1: 0.94391
epoch 4/5 time/epoch=96.00000 sec; time left=0.0267 hr; avg loss=0.13408; f1=0.96033
epoch 4/5 validation loss: 0.171059  f1: 0.94954
{% endhighlight %}

As one can see, after 5 epochs cross-entropy error decreased from 6.7 to 0.17.
At the same time accuracy F1 score has incerased from initial 0.05 (5% of accurate class predictions)
to 0.96 (96% of accurate classifications).

C++ implementation is available at [dense7.cpp] [cpp_source_code], [mnist.h] [mnist_source_code], and [f1.h] [f1_source_code]


[part_1]: https://alexgl-github.github.io/github/jekyll/2021/05/21/CCE.html

[part_2]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Softmax.html

[part_3]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Sigmoid.html

[part_4]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Dense_layer_with_bias.html

[part_5]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Multiple_Dense_layers.html

[part_6]: https://alexgl-github.github.io/github/jekyll/2021/05/21/Dense_layer.html

[MNIST]: http://yann.lecun.com/exdb/mnist/

[cpp_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/dense7.cpp

[mnist_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/mnist.h

[f1_source_code]:  https://github.com/alexgl-github/alexgl-github.github.io/tree/main/src/f1.h

