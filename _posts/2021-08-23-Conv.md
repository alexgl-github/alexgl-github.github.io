---
layout: post
mathjax: true
title:  "DNN with backpropagation in C++, part 8"
date:   2021-08-23 00:00:00 +0000
categories: github jekyll
---

### Implementing convolution layer in C++

## Definitions

Continuos convolution of two functions f and g is defined as:

$$
 (f * g)(t) = \int_{-\infty}^{\infty} f(\mathcal{\tau})g(t-\mathcal{\tau}) \partial \mathcal{\tau}
$$

Discrete convolution is defined by the sum:

$$
 (f * g)(i) = \sum_{m=-\infty}^{\infty} f(i-m) g(m)
$$

Discrete convolution of finite length f and g is:

$$
 y(i) = \sum_{m=0}^{M-1} f(i-m) g(m)
$$

For 2-D case, discrete convolution h(i,j) is:

$$
 f(i, j) = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} f(i-m, j-n) g(m, n)
$$

### Example of discrete 2D convolution of 5x5 input (light blue) with 3x3 kernel (dark blue), producing 3x3 output (green)

Kernel:

![conv_kernel_image]({{ site.url }}/images/kernel1.png)

Convolution:

![conv]({{ site.url }}/images/conv1.png)

_Convolution illustration is from "A guide to convolution arithmetic for deep learning", by Vincent Dumoulin, Francesco Visin_


### Forward path for convolution layer

Convolution layer output $$ \hat Y $$ is:

$$
\hat Y = W * X
$$

W is convolution kernel of size K:

$$
W^\intercal = \left( \begin{array}{ccc}
w_{0} & w_{1} & \ldots & x_{K-1} \\
\end{array} \right)
$$

X is input vector of size N:

$$
X^\intercal = \left( \begin{array}{ccc}
x_{0} & x_{1} & \ldots & w_{N-1} \\
\end{array} \right)
$$



### Convolution as a matrix multiplication

1D convolution operator can be represented as a matrix multiplication of [Toeplitz matrix] [toeplitz_matrix] representation of kernel weights $$ T_{w} $$, and input X:

$$
\hat Y = T_{w} X = \left( \begin{array}{ccc}
 w_{0}    &      0    &  \ldots &        0   \\
 w_{1}    &  w_{0}    &  \ldots &   \vdots   \\
\vdots    &  w_{1}    &  \ldots &    w_{0}   \\
 w_{m-1}  & \vdots    &  \ldots &   \vdots   \\
     0    &  w_{m-1}  &  \ldots &  w_{m-2}   \\
     0    &      0    &  \ldots &  w_{m-1}   \\
\end{array} \right) \left( \begin{array}{ccc}
x_{0}   \\
x_{1}   \\
\ldots  \\
x_{n-1} \\
\end{array} \right)
$$

Convolution can also be represented as matrix multiplication of kernel weights W and Toeplitz matrix of input X

$$
\hat Y = W^\intercal T_{x} = \left( \begin{array}{ccc}
w_{0} & w_{1} & \ldots & x_{k-1} \\
\end{array} \right) \left( \begin{array}{ccc}
x_{0}  &  x_{1}  &  \ldots &  x_{n-1}  &          0 &        0  \\
    0  &  x_{0}  &   x_{1} &   \ldots  &    x_{n-1} &        0  \\
\vdots & \vdots  &  \ldots &   \ldots  &     \ldots &   \ldots  \\
   0   &      0  &       0 &   \ldots  &    x_{n-2} &   x_{n-1} \\
\end{array} \right)
$$

### Error backpropagation

#### For input $$X$$, we want to minimize the error difference between out network output and expected output,
by adjusting convolution layer weights by error E gradient $$\frac {\partial E(\hat Y)} {\partial W}$$


Using chain rule:

$$
\frac {\partial E } {\partial W} = \frac {\partial E} {\partial \hat Y} \frac {\partial \hat Y} {\partial W}
$$

Using Jacobian $$ J = \frac {\partial E} {\partial \hat Y} $$:

$$
\frac {\partial E} {\partial \hat Y} = \left( \begin{array}{ccc}
\partial \hat y_{0} & \partial \hat y_{1} & \ldots & \partial \hat y_{n-1} \\
\end{array} \right)
$$


And using Toeplitz matrix multiplication for Y, convolution, we can see that partial derivative
of convolution $$ \frac {\partial \hat Y} {\partial W} $$ is tansposed matrix $$ T_{x} $$:

$$
\frac {\partial \hat Y} {\partial W} = \frac {\partial (W * X)} {\partial W}  = \frac {\partial (W T_{x})} {\partial W} = T_{x}^\intercal
$$

Kernel weight gradient $$ \frac {\partial E} {\partial W} $$ becomes convolution of Conv layer input $$ X $$ and filter $$ \frac {\partial E} {\partial \hat Y} $$ :

$$
\frac {\partial E} {\partial W} = J T_{x}^\intercal = \left( \begin{array}{ccc}
\partial \hat y_{0} & \partial \hat y_{1} & \ldots & \partial \hat y_{n-1} \\
\end{array} \right)  \left( \begin{array}{ccc}
x_{0}    &      0    &  \ldots &        0   \\
x_{1}    &  x_{0}    &  \ldots &        0   \\
\vdots   &  x_{1}    &  \ldots &    x_{0}   \\
x_{n-1}  & \ldots    &  \ldots &   \ldots   \\
   0     &  x_{n-1}  &  \ldots &  x_{n-2}   \\
   0     &      0    &  \ldots &  x_{n-1}   \\
\end{array} \right)
$$

Shorter form for this:

$$
 \frac {\partial E (\hat W) } {\partial W} = \frac {\partial E} {\partial \hat Y} * X
$$


### We also need to find output gradient update $$ \frac {\partial E (\hat Y) } {\partial X} $$ used in backpropagation for the previous layer

$$
\frac {\partial E (\hat Y) } {\partial X} = \frac {\partial E} {\partial \hat Y} * \frac {\partial \hat Y} {\partial X}
$$


Replacing convolution with multilication by Toeptlitz for kernel weights:

$$
\frac {\partial \hat Y} {\partial X} = \frac {\partial (T_{w} X)} {\partial X} = T_{w}
$$

$$
\frac {\partial E (\hat Y) } {\partial X} = J T_{w}^\intercal = \left(  \begin{array}{ccc}
\partial \hat y_{0}  & \partial \hat y_{1} & \ldots  & \partial \hat y_{n} \\
\end{array} \right)  \left( \begin{array}{ccc}
 w_{0}    &      0    &  \ldots &        0   \\
 w_{1}    &  w_{0}    &  \ldots &   \vdots   \\
\vdots    &  w_{1}    &  \ldots &    w_{0}   \\
 w_{m-1}  & \vdots    &  \ldots &   \vdots   \\
     0    &  w_{m-1}  &  \ldots &  w_{m-2}   \\
     0    &      0    &  \ldots &  w_{m-1}   \\
\end{array} \right)
$$

### Output gradient update is convolution of error gradient J and kernel weighs rotated 180 degrees

$$
\frac {\partial E (\hat Y) } {\partial X} = J * ROT_{180}(W)
$$

### Bias

TBD

### Backpropagation for 2D convolution layer

Backpropagation for 2D convolution layer is similar to the 1D cas, except doubly block Toeplitz matrices are used for input and kernel representation.

### Multiple input and output channels

Convolution of two channel input producing tree channel output

![conv]({{ site.url }}/images/multichannel_conv.png)

illustration of multiple channel convolution is from "A guide to convolution arithmetic for deep learning", by Vincent Dumoulin, Francesco Visin.

In case of multiple input channels , convolution channel output is a sum of convolutions for each input channel.

From basic differentiation rules,  derivative of a sum of functions is the sum of their derivatives.
Output gradient and convoluton kernel weight update will be sums of corresponding dervatives over output channels.

### Let's implement single convolution layer DNN

#### Python implementation will be used for validating C++ convolution code in the following section

{% highlight python %}

import tensorflow as tf
from tensorflow.keras.layers import Dense, Softmax
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import backend as K
import numpy as np
np.set_printoptions(formatter={'float': '{: 0.3f}'.format}, edgeitems=10, linewidth=180)

# Convolution parameters used in this example
# 5x5 input plane
input_height = 5
input_width = 5
# 1 channel input
channels_in = 1
# 2 chnnel output
channels_out = 2
# 3x3 kernel size
kernel_size = 3
# stride of 1
stride = 1

# conv layer weights initializer is [1, 2, ..., kernel_size * kernel_size * channels_in * channels_out]
kernel_weights_num = kernel_size * kernel_size * channels_in * channels_out
conv_weights = np.reshape(np.linspace(start=1, stop=kernel_weights_num, num=kernel_weights_num),
                          (channels_out, channels_in, kernel_size, kernel_size))

# conv layer bias initializer is array [channels_out, channels_out-1, ..., 1]
conv_bias = np.linspace(start=channels_out, stop=1, num=channels_out)

# Conv layer weights are in Height, Width, Input channels, Output channels (HWIO) format
conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])

# generate input data
input_shape = (1, input_height, input_width, channels_in)
input_size = input_height * input_width * channels_in
x = np.reshape(np.linspace(start = input_size , stop = 1, num = input_size),
               (1, channels_in, input_height, input_width))

# input data is in Batch, Height, Width, Channels (BHWC) format
x = np.transpose(x, [0, 2, 3, 1])


# Create sequential model with 1 conv layer
# Conv layer has bias, no activation stride of 1, 3x3 kernel, zero input padding
# for output to have same dimension as input
model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=channels_out,
                                 kernel_size=kernel_size,
                                 strides=stride,
                                 activation=None,
                                 use_bias=True,
                                 data_format="channels_last",
                                 padding="same",
                                 weights=[conv_weights, conv_bias]))

# Builds the model based on input shapes received
model.build(input_shape=input_shape)

# Use MSE for loss function
loss_fn = tf.keras.losses.MeanSquaredError()

# print input data in BCHW format
print(f"input x:\n{np.squeeze(np.transpose(x, [0, 3, 1, 2]))}")

# Print Conv kernel in OIHW format
print(f"conv kernel weights:\n {np.transpose(model.trainable_variables[0].numpy(), [3, 2, 0, 1])}")

# print Conv bias
print(f"conv kernel bias: {model.trainable_variables[1].numpy()}")

# Create expected output
y_true = np.ones(shape=(1, input_height, input_width, channels_out))

# SGD update rule for parameter w with gradient g when momentum is 0:
# w = w - learning_rate * g
# For simplicity make learning_rate=1.0
optimizer = tf.keras.optimizers.SGD(learning_rate=1.0, momentum=0.0)

# Get model output y for input x, compute loss, and record gradients
with tf.GradientTape(persistent=True) as tape:
    xt = tf.convert_to_tensor(x)
    tape.watch(xt)
    y = model(xt)
    loss = loss_fn(y_true, y)


# dloss_dy is error gradient w.r.t. DNN output y
dloss_dy = tape.gradient(loss, y)

# dloss_dx is error gradient w.r.t DNN input x
dloss_dx = tape.gradient(loss, xt)

# Update DNN weights with gradients
grad = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(grad, model.trainable_variables))

# print model output in BCHW format
print(f"output y:\n{np.squeeze(np.transpose(y, [0, 3, 1, 2]))}")

# print loss
print("loss: {}".format(loss))

# print dloss_dy: error gradient w.r.t. DNN output y, in BCHW format
print("dloss_dy:\n{}".format(np.squeeze((np.transpose(dloss_dy, [0, 3, 1, 2])))))

# print dloss_dx: error gradient w.r.t DNN input x, , in BCHW format
print("dloss_dx:\n{}".format(np.squeeze(np.transpose(dloss_dx, [0, 3, 1, 2]))))

# print updated conv layer kernel and bias weights
print(f"updated conv kernel:\n {np.transpose(model.trainable_variables[0].numpy(), [3, 2, 0, 1])}")
print(f"updated conv bias: {model.trainable_variables[1].numpy()}")

{% endhighlight %}


### Output after running Python code:

{% highlight bash %}

$ python3 conv8.py
input x:
[[ 25.000  24.000  23.000  22.000  21.000]
 [ 20.000  19.000  18.000  17.000  16.000]
 [ 15.000  14.000  13.000  12.000  11.000]
 [ 10.000  9.000  8.000  7.000  6.000]
 [ 5.000  4.000  3.000  2.000  1.000]]
conv kernel weights:
 [[[[ 1.000  2.000  3.000]
   [ 4.000  5.000  6.000]
   [ 7.000  8.000  9.000]]]


 [[[ 10.000  11.000  12.000]
   [ 13.000  14.000  15.000]
   [ 16.000  17.000  18.000]]]]
conv kernel bias: [ 2.000  1.000]
output y:
[[[ 602.000  814.000  775.000  736.000  442.000]
  [ 584.000  761.000  716.000  671.000  386.000]
  [ 419.000  536.000  491.000  446.000  251.000]
  [ 254.000  311.000  266.000  221.000  116.000]
  [ 98.000  112.000  91.000  70.000  34.000]]

 [[ 1393.000  1974.000  1881.000  1788.000  1125.000]
  [ 1636.000  2299.000  2173.000  2047.000  1276.000]
  [ 1201.000  1669.000  1543.000  1417.000  871.000]
  [ 766.000  1039.000  913.000  787.000  466.000]
  [ 349.000  462.000  387.000  312.000  177.000]]]
loss: 1025531.8125
dloss_dy:
[[[ 24.040  32.520  30.960  29.400  17.640]
  [ 23.320  30.400  28.600  26.800  15.400]
  [ 16.720  21.400  19.600  17.800  10.000]
  [ 10.120  12.400  10.600  8.800  4.600]
  [ 3.880  4.440  3.600  2.760  1.320]]

 [[ 55.680  78.920  75.200  71.480  44.960]
  [ 65.400  91.920  86.880  81.840  51.000]
  [ 48.000  66.720  61.680  56.640  34.800]
  [ 30.600  41.520  36.480  31.440  18.600]
  [ 13.920  18.440  15.440  12.440  7.040]]]
dloss_dx:
[[ 3771.400  6172.440  6685.680  5724.120  3620.520]
 [ 6228.040  10083.239  10847.159  9220.200  5771.960]
 [ 5474.880  8779.320  9310.321  7791.479  4815.600]
 [ 3585.160  5672.041  5873.400  4784.041  2896.760]
 [ 1824.040  2832.360  2838.720  2224.680  1306.440]]
updated conv kernel:
 [[[[-4303.520 -5192.000 -4407.840]
   [-5828.240 -6937.000 -5822.080]
   [-4193.120 -4923.601 -4062.880]]]


 [[[-13732.480 -16211.801 -13576.799]
   [-16576.041 -19387.197 -16083.959]
   [-11717.280 -13513.800 -11024.640]]]]
updated conv bias: [-405.120 -1196.040

{% endhighlight %}

### C++ implementation

{% highlight c++ %}

#include <cstdio>
#include <vector>
#include <array>
#include <algorithm>
#include <array>
#include <iterator>
#include <variant>
#include <random>

using namespace std;

/*
 * Print helper function
 */
auto print_n(const float& x, const char* fmt)
{
  printf(fmt, x);
}

template<typename T>
auto print_n(T& x, const char* fmt="%7.2f ")
{
  std::for_each(x.begin(), x.end(),
                [fmt](const auto& xi)
                {
                  print_n(xi, fmt);
                });
  printf("\n");
}


/*
 *
 */
template<typename G>
auto initialize(float& x, G& generator)
{
  x = generator();
}

template<typename T, typename G>
auto initialize(T& x, G& generator)
{
  std::for_each(x.begin(), x.end(), [generator](auto& x_i)
                {
                  initialize(x_i, generator);
                });
}


/*
 * Incremental and decremental initializers
 */
template<int initial_value = 0, typename T=float>
auto gen_inc = []() { static int i = initial_value; return static_cast<T>(i++);};

template<int initial_value = 0, typename T=float>
auto gen_dec = []() { static int i = initial_value; return static_cast<T>(i--);};

/*
 * Constant weight intializer
 */

template<typename T = float, const int num = 0, const int det = 1>
constexpr auto const_initializer = []() -> float
{
  return static_cast<T>(num) / static_cast<T>(det);
};

/*
 * Random uniform weights initializer
 */
constexpr auto random_uniform_initializer = []() -> float
{
  /*
   * Return random values in the range [-1.0, 1.0]
   */
  return 2.0 * static_cast<float>(rand()) / static_cast<float>(RAND_MAX) - 1.0;
};

/*
 * Mean Squared Error loss class
 * Parameters:
 *  num_inputs: number of inputs to MSE function.
 *  T: input type, float by defaut.
 */
template<size_t num_inputs, typename T = float>
struct MSE
{
  /*
   * Forward pass computes MSE loss for inputs y (label) and yhat (predicted)
   */
  static T forward(const array<T, num_inputs>& y, const array<T, num_inputs>& yhat)
  {
    T loss = transform_reduce(y.begin(), y.end(), yhat.begin(), 0.0, plus<T>(),
                              [](const T& left, const T& right)
                              {
                                return (left - right) * (left - right);
                              }
                              );
    return loss / static_cast<T>(num_inputs);
  }

  /*
   * Backward pass computes dloss/dy for inputs y (label) and yhat (predicted)
   *
   * loss = sum((yhat[i] - y[i])^2) / N
   *   i=0...N-1
   *   where N is number of inputs
   *
   * d_loss/dy[i] = 2 * (yhat[i] - y[i]) * (-1) / N
   * d_loss/dy[i] = 2 * (y[i] - yhat[i]) / N
   *
   */
  static array<T, num_inputs> backward(const array<T, num_inputs>& y, const array<T, num_inputs>& yhat)
  {
    array<T, num_inputs> de_dy;

    transform(y.begin(), y.end(), yhat.begin(), de_dy.begin(),
              [](const T& left, const T& right)
              {
                return 2 * (right - left) / num_inputs;
              }
              );
    return de_dy;
  }

};

template<std::size_t input_height,
         std::size_t input_width,
         std::size_t channels,
         typename T = float>
struct Flatten
{
  typedef array<T, input_width> input_row;
  typedef array<input_row, input_height> input_plane;
  typedef array<input_plane, channels> input_type;
  typedef array<T, input_width * input_height * channels> output_type;

  output_type forward(const input_type& x)
  {
    output_type y;
    size_t idx = 0;

    for (size_t i = 0; i < input_height; i++)
      {
        for (size_t j = 0; j < input_width; j++)
          {
            for (size_t channel = 0; channel < channels; channel++)
              {
                y[idx++] = x[channel][i][j];
              }
          }
      }
    return y;
  }

  input_type backward(const output_type& grad)
  {
    input_type grad_out;
    size_t idx = 0;

    for (size_t i = 0; i < input_height; i++)
      {
        for (size_t j = 0; j < input_width; j++)
          {
            for (size_t channel = 0; channel < channels; channel++)
              {
                grad_out[channel][i][j] = grad[idx++];
              }
          }
      }
    return grad_out;
  }
};


/*
 *
 */
template<int input_height,
         int input_width,
         int channels_inp = 1,
         int channels_out =1,
         int kernel_size = 3,
         int stride = 1,
         bool use_bias = false,
         typename T = float,
         T (*weights_initializer)() = const_initializer<>,
         T (*bias_initializer)() = const_initializer<>>
struct Conv2D
{
  typedef array<T, input_width> input_row;
  typedef array<input_row, input_height> input_plane;
  typedef array<input_plane, channels_inp> conv_input;

  static const int output_width = input_width;
  static const int output_height = input_height;

  typedef array<T, output_width> output_row;
  typedef array<output_row, output_height> output_plane;
  typedef array<output_plane, channels_out> conv_output;

  typedef array<array<T, kernel_size>, kernel_size> conv_kernel;
  /*
   * OIHW
   */
  typedef array<array<conv_kernel, channels_inp>, channels_out> conv_weights;

  typedef array<T, channels_out> conv_bias;

  conv_weights weights;
  conv_bias bias;
  conv_weights dw;
  conv_bias db;

  static const int pad_size = kernel_size / 2;

  Conv2D()
  {
    initialize(weights, *weights_initializer);
    initialize(bias, *bias_initializer);
    initialize(dw, const_initializer<>);
    initialize(db, const_initializer<>);
  }

  template<int height_x, int width_x, int height_w, int width_w>
  static T conv (const std::array<std::array<T, width_x>, height_x>& x,
          const std::array<std::array<T, width_w>, height_w>& w,
          int i, int j)
  {
    const int pad_top = (i < 0) ? (-i) : 0;
    const int pad_bot = (i > height_x - height_w) ? (i - height_x + height_w) : 0;
    const int pad_left = (j < 0) ? (- j) : 0;
    const int pad_right = (j > width_x - width_w) ? (j - width_x + width_w) : 0;

    T sum = std::transform_reduce(w.begin() + pad_top,
                                  w.end()   - pad_bot,
                                  x.begin() + pad_top + i,
                                  static_cast<T>(0),
                                  std::plus<T>(),
                                  [j, pad_left, pad_right](auto& w_i, auto& x_i) -> T
                                    {
                                      return std::inner_product(w_i.begin() + pad_left,
                                                                w_i.end()   - pad_right,
                                                                x_i.begin() + pad_left + j,
                                                                static_cast<T>(0));
                                    }
                                  );

    return sum;
  };

  conv_output forward(const conv_input& x)
  {
    conv_output y;

    for (int output_channel = 0; output_channel < channels_out; output_channel++)
      {
        for (int i = 0; i < output_height; i++)
          {
            for (int j = 0; j < output_width; j++)
              {
                y[output_channel][i][j] = use_bias * bias[output_channel];
                for (int input_channel = 0; input_channel < channels_inp; input_channel++)
                  {
                    y[output_channel][i][j] +=
                      conv<input_height, input_width, kernel_size, kernel_size>(x[input_channel],
                                                                                weights[output_channel][input_channel],
                                                                                i - pad_size,
                                                                                j - pad_size);
                  }
              }
          }
      }
    return y;
  }

  conv_input backward(const conv_input& x,  const conv_output& grad)
  {
    conv_input grad_out = {};

    for (int output_channel = 0; output_channel < channels_out; output_channel++)
      {
        for (int i = 0; i < kernel_size; i++)
          {
            for (int j = 0; j < kernel_size; j++)
              {
                for (int input_channel = 0; input_channel < channels_inp; input_channel++)
                  {
                    dw[output_channel][input_channel][i][j] +=
                      conv<input_height, input_width, output_height, output_width>(x[input_channel],
                                                                                   grad[output_channel],
                                                                                   i - pad_size,
                                                                                   j - pad_size);
                  }
              }
          }
      }

    for (int output_channel = 0; output_channel < channels_out; output_channel++)
      {
        db[output_channel] += std::accumulate(grad[output_channel].cbegin(), grad[output_channel].cend(),
                                              static_cast<T>(0),
                                              [](auto total, const auto& grad_i)
                                              {
                                                return std::accumulate(grad_i.cbegin(), grad_i.cend(), total);
                                              });
      }

    conv_weights weights_rot180;
    for (int input_channel = 0; input_channel < channels_inp; input_channel++)
      {
        for (int output_channel = 0; output_channel < channels_out; output_channel++)
          {

            for (size_t i = 0; i < kernel_size; i++)
              {
                for (size_t j = 0; j < kernel_size; j++)
                  {
                    weights_rot180[output_channel][input_channel][i][j] = weights[output_channel][input_channel][i][kernel_size - j - 1];
                  }
              }

            for (size_t i = 0; i < kernel_size/2; i++)
              {
                for (size_t j = 0; j < kernel_size; j++)
                  {
                    auto t = weights_rot180[output_channel][input_channel][i][j];
                    weights_rot180[output_channel][input_channel][i][j] = weights_rot180[output_channel][input_channel][kernel_size - i - 1][j];
                    weights_rot180[output_channel][input_channel][kernel_size - i - 1][j] = t;
                  }
              }

          }
      }

    for (int input_channel = 0; input_channel < channels_inp; input_channel++)
      {
        for (int i = 0; i < input_height; i++)
          {
            for (int j = 0; j < input_width; j++)
              {
                grad_out[input_channel][i][j] = 0.0;
                for (int output_channel = 0; output_channel < channels_out; output_channel++)
                  {
                    grad_out[input_channel][i][j] +=
                      conv<output_height, output_width, kernel_size, kernel_size>(grad[output_channel],
                                                                                  weights_rot180[output_channel][input_channel],
                                                                                  i - pad_size,
                                                                                  j - pad_size);
                  }
              }
          }
      }

    return grad_out;
  }

  void train(float learning_rate)
  {
    /*
     * compute w = w - learning_rate * dw
     */
    for (int input_channel = 0; input_channel < channels_inp; input_channel++)
      {
        for (int output_channel = 0; output_channel < channels_out; output_channel++)
          {
            for (size_t i = 0; i < kernel_size; i++)
              {
                for (size_t j = 0; j < kernel_size; j++)
                  {
                    weights[output_channel][input_channel][i][j] = weights[output_channel][input_channel][i][j] - learning_rate * dw[output_channel][input_channel][i][j];
                  }
              }
          }
      }

    /*
     * compute bias = bias - learning_rate * db
     */
    if (use_bias)
      {
        for (int output_channel = 0; output_channel < channels_out; output_channel++)
          {
            bias[output_channel] -= learning_rate * db[output_channel];
          }
      }

    /*
     * Reset accumulated dw and db
     */
    reset_gradients();
  }

  /*
   * Reset weigth and bias gradient accumulators
   */
  void reset_gradients()
  {
    initialize(dw, const_initializer<>);
    initialize(db, const_initializer<>);
  }

};

/*
 * DNN train and validation loops are implemented in the main() function.
 */
int main(void)
{
  const int input_height = 5;
  const int input_width = 5;
  const int channels_in = 1;
  const int channels_out = 2;
  const int kernel_size = 3;
  std::array<std::array<std::array<float, input_width>, input_height>, channels_in> x = {};

  initialize(x, gen_dec<input_height * input_width * channels_in>);

  std::array<float, input_height * input_width * channels_out>  y_true;
  std::fill(y_true.begin(), y_true.end(), 1.0);

  /*
   * Create DNN layers and the loss
   */
  Conv2D<input_height,         /* input height */
         input_width,          /* input width */
         channels_in,          /* number of input channels */
         channels_out,         /* number of output channels */
         kernel_size,          /* convolution kernel size */
         1,                    /* stride */
         true,                 /* use_bias flag */
         float,                /* conv data type */
         gen_inc<1>,           /* initialier for kernel weights */
         gen_dec<channels_out> /* initialier for bias weights */
         > conv;
  Flatten<input_height, input_width, channels_out> flatten;
  MSE<input_height * input_width * channels_out> loss_fn;

  printf("input x:\n");
  print_n(x);

  printf("conv weights:\n");
  print_n(conv.weights);
  printf("conv bias:");
  print_n(conv.bias);

  auto y1 = conv.forward(x);
  auto y  = flatten.forward(y1);
  auto loss = loss_fn.forward(y_true, y);

  printf("output y:\n");
  print_n(y1);

  printf("loss: %.5f\n", loss);

  auto dloss_dy  = loss_fn.backward(y_true, y);
  auto dloss_dy1 = flatten.backward(dloss_dy);
  auto dloss_dx  = conv.backward(x, dloss_dy1);

  printf("dloss_dy:\n");
  print_n(dloss_dy1);

  printf("dloss_dx:\n");
  print_n(dloss_dx);

  conv.train(/*learning_rate */ 1.0);

  printf("updated conv weights:\n");
  print_n(conv.weights);

  printf("updated conv bias:\n");
  print_n(conv.bias);

  return 0;
}

{% endhighlight %}


#### Output after running C++ code matches Python/Tensorflow output from the previous section.

{% highlight bash %}

$ g++ -o conv8 -Wall -std=c++2a conv8.cpp && ./conv8
input x:
  25.00   24.00   23.00   22.00   21.00
  20.00   19.00   18.00   17.00   16.00
  15.00   14.00   13.00   12.00   11.00
  10.00    9.00    8.00    7.00    6.00
   5.00    4.00    3.00    2.00    1.00


conv weights:
   1.00    2.00    3.00
   4.00    5.00    6.00
   7.00    8.00    9.00


  10.00   11.00   12.00
  13.00   14.00   15.00
  16.00   17.00   18.00



conv bias:   2.00    1.00
output y:
 602.00  814.00  775.00  736.00  442.00
 584.00  761.00  716.00  671.00  386.00
 419.00  536.00  491.00  446.00  251.00
 254.00  311.00  266.00  221.00  116.00
  98.00  112.00   91.00   70.00   34.00

1393.00 1974.00 1881.00 1788.00 1125.00
1636.00 2299.00 2173.00 2047.00 1276.00
1201.00 1669.00 1543.00 1417.00  871.00
 766.00 1039.00  913.00  787.00  466.00
 349.00  462.00  387.00  312.00  177.00


loss: 1025531.81250
dloss_dy:
  24.04   32.52   30.96   29.40   17.64
  23.32   30.40   28.60   26.80   15.40
  16.72   21.40   19.60   17.80   10.00
  10.12   12.40   10.60    8.80    4.60
   3.88    4.44    3.60    2.76    1.32

  55.68   78.92   75.20   71.48   44.96
  65.40   91.92   86.88   81.84   51.00
  48.00   66.72   61.68   56.64   34.80
  30.60   41.52   36.48   31.44   18.60
  13.92   18.44   15.44   12.44    7.04


dloss_dx:
3771.40 6172.44 6685.68 5724.12 3620.52
6228.04 10083.24 10847.16 9220.20 5771.96
5474.88 8779.32 9310.32 7791.48 4815.60
3585.16 5672.04 5873.40 4784.04 2896.76
1824.04 2832.36 2838.72 2224.68 1306.44


updated conv weights:
-4303.52 -5192.00 -4407.84
-5828.24 -6937.00 -5822.08
-4193.12 -4923.60 -4062.88


-13732.48 -16211.80 -13576.80
-16576.04 -19387.20 -16083.96
-11717.28 -13513.80 -11024.64



updated conv bias:
-405.12 -1196.04

{% endhighlight %}

[toeplitz_matrix]: https://en.wikipedia.org/wiki/Toeplitz_matrix


