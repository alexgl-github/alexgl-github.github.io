---
layout: post
mathjax: true
title:  "DNN with backpropagation in C++, part 8"
date:   2021-08-23 00:00:00 +0000
categories: github jekyll
---

## Implementing convolution layer in C++

Convolution is a mathematical operation on two functions f and g that produces a function $$f*g$$

$$
 (f * g)(t) = \int_{-\infty}^{\infty} f(\mathcal{t})g(t-\mathcal{t}) \partial \mathcal{t}
$$

Discrete convolution is an operation on two discrete time signals defined by the sum:

$$
 (f * g)(i) = \sum_{m=-\infty}^{\infty} f(i-m) g(m)
$$

Discrete convolution with filter size M, this can be written as:

$$
 y(i) = \sum_{m=0}^{M-1} f(i-m) g(m)
$$

For 2-D case, discrete convolution h(i,j) is:

$$
 f(i, j) = \sum_{m=0}^{M-1} \sum_{n=0}^{N} f(i-m, j-n) g(m, n)
$$

### Example of discrete 2D convolution of input featue map (light blue) with kernel (dark blue)

Kernel:

![conv_kernel_image]({{ site.url }}/images/kernel1.png)

Convolution:

![conv]({{ site.url }}/images/conv1.png)

Convolution illustration is from "A guide to convolution arithmetic for deep learning", by Vincent Dumoulin, Francesco Visin. arXiv:1603.07285


### Forward path for convolution layer

Convolution layer neural net output $$\hat Y$$ is:

$$
\hat Y = W * X
$$

where X is input vector of size N:

$$
X = \left( \begin{array}{ccc}
x_{1} & x_{2} & \ldots & w_{N} \\
\end{array} \right)
$$

W is convolution kernel of size K

$$
W^T = \left( \begin{array}{ccc}
w_{1} & w_{2} & \ldots & x_{K} \\
\end{array} \right)
$$


### Convolution as a matrix multiplication

1D convolution operator can be represented as a matrix multiplication of Toeplitz matrix of kernel weights and transposed input X
$$
\hat Y = T_{w} X^t
$$

Convolution kernel W is converted into [Toeplitz matrix] [toeplitz_matrix] $$ T_{w} $$

$$
T_{w} = \left( \begin{array}{ccc}
 w_{1}  &      0  &  \ldots &        0  &      0  \\
 w_{2}  &  w_{1}  &  \ldots &        0  &      0  \\
\vdots  &  w_{2}  &  \ldots &   \vdots  &      0  \\
 w_{m}  & \vdots  &  \ldots &        0  & \vdots  \\
     0  &  w_{m}  &  \ldots &    w_{1}  &      0  \\
     0  &      0  &  \ldots &    w_{2}  &  w_{1}  \\
\end{array} \right)
$$

Also, convolution can be represented as matrix multiplication of kernel weights W and Toeplitz matrix of input X

$$
\hat Y = W T_{x}
$$

where input X converted into [Toeplitz matrix] [toeplitz_matrix]

$$
T_{x} = \left( \begin{array}{ccc}
x_{1}  &  x_{2}  &  \ldots &  x_{n} &        0 &        0  \\
    0  &  x_{1}  &   x_{2} & \ldots &    x_{n} &        0  \\
\vdots & \vdots  &  \ldots & \ldots &   \ldots &   \ldots  \\
   0   &      0  &   x_{0} & \ldots &  x_{n-1} &    x_{N}  \\
\end{array} \right)
$$

### Error backpropagation

#### For input $$X$$, we want to minimize the error difference between out network output and expected output,
by adjusting convolution layer weights by error gradient $$\frac {\partial E} {\partial W}$$

Using chain rule:

$$
\frac {\partial E (\hat Y) } {\partial W} = \frac {\partial E} {\partial \hat Y} * \frac {\partial \hat Y} {\partial W}
$$

Partial derivative of error E with respect to Y is a Jacobian:

$$
J = \frac {\partial E} {\partial \hat Y} =  \left( \frac {\partial E} {\partial \hat y_{1}}, \frac {\partial E} {\partial \hat y_{2}},  \vdots \frac {\partial E} {\partial \hat y_{N} } \right)
$$


Partial derivative of Y with respect to weights:

$$
\frac {\partial \hat Y} {\partial W} = \frac {\partial (X * W)} {\partial W}
$$

Replacing convolution with multiplication:

$$
\frac {\partial \hat Y} {\partial W} = \frac {\partial (W T_{x})} {\partial W} = T_{x}^T
$$

$$
\frac {\partial \hat Y} {\partial W} = T_{x}^T
$$

Also using Jacobian for $$ \frac {\partial E} {\partial \hat Y}  $$ :

$$
\frac {\partial E (\hat Y) } {\partial W} = J T_{x}^T
$$

One can notice that $$ \frac {\partial E (\hat Y) } {\partial W} $$ is convolution of error derivative J with respect to Y,  and convolution input X

$$
J T_{x}^t = J * X
$$


#### We also need to find output gradient update $$ \frac {\partial E (\hat Y) } {\partial X} $$ used in backpropagation of the previous layer

$$
\frac {\partial E (\hat Y) } {\partial X} = \frac {\partial E} {\partial \hat Y} * \frac {\partial \hat Y} {\partial X}
$$


Replacing convolution with multilication by Toeptlitz for kernel weights:

$$
\frac {\partial \hat Y} {\partial X} = \frac {\partial (X T_{w})} {\partial X} = T_{w}^t
$$

$$
\frac {\partial E (\hat Y) } {\partial X} = J T_{w}^t
$$

One can notice that output gradient update is convolution of erro gradient J and kernel weighs rotated 180 degrees

$$
\frac {\partial E (\hat Y) } {\partial X} = J * ROT_{180}(W)
$$

[toeplitz_matrix]: https://en.wikipedia.org/wiki/Toeplitz_matrix


